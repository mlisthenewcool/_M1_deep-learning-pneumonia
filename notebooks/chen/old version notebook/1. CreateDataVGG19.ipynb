{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. CreateDataVGG19.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"qhG8deumhkP4","outputId":"72eda17f-ae46-45e4-a093-a3d6adad4129","executionInfo":{"status":"ok","timestamp":1557072726741,"user_tz":-120,"elapsed":1335,"user":{"displayName":"CHEN DANG","photoUrl":"https://lh4.googleusercontent.com/-ULtn-wp1jmI/AAAAAAAAAAI/AAAAAAAAAGY/2jasw42jxC8/s64/photo.jpg","userId":"10929130588901207444"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AdMtzQw8hd2n","outputId":"8ac5df4f-acdb-4ca9-d6d9-75f4654b360c","executionInfo":{"status":"ok","timestamp":1557072750089,"user_tz":-120,"elapsed":2865,"user":{"displayName":"CHEN DANG","photoUrl":"https://lh4.googleusercontent.com/-ULtn-wp1jmI/AAAAAAAAAAI/AAAAAAAAAGY/2jasw42jxC8/s64/photo.jpg","userId":"10929130588901207444"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten, ActivityRegularization, Lambda\n","from keras.layers import Convolution2D, MaxPooling2D, Conv1D, MaxPooling1D, TimeDistributed\n","from keras.layers import AveragePooling2D, Input\n","from keras.utils import np_utils, normalize\n","from keras.engine import InputLayer\n","from keras import backend as K\n","\n","import os\n","import scipy.io as sio\n","import h5py\n","import glob\n","import time\n","\n","import sklearn\n","from sklearn.preprocessing import normalize\n","\n","from keras.applications.vgg19 import VGG19\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing import image\n","from keras.applications.vgg19 import preprocess_input\n","from keras.models import Model\n","from sklearn.linear_model import SGDClassifier\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qA-lUBL_pCbg","outputId":"9e64d53f-5ead-48f5-b348-745f115e5a69","executionInfo":{"status":"ok","timestamp":1557072757872,"user_tz":-120,"elapsed":4922,"user":{"displayName":"CHEN DANG","photoUrl":"https://lh4.googleusercontent.com/-ULtn-wp1jmI/AAAAAAAAAAI/AAAAAAAAAGY/2jasw42jxC8/s64/photo.jpg","userId":"10929130588901207444"}},"colab":{"base_uri":"https://localhost:8080/","height":1131}},"source":["network = VGG19(weights='imagenet')#, input_shape=(fscale, fscale, 3))\n","print(network.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 4096)              102764544 \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 4096)              16781312  \n","_________________________________________________________________\n","predictions (Dense)          (None, 1000)              4097000   \n","=================================================================\n","Total params: 143,667,240\n","Trainable params: 143,667,240\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LnZbOJt1gNo9","colab":{}},"source":["#dataset's path \n","dir_path_DataSet = 'drive/My Drive/medical_image_recognition/datasets/'\n","\n","#path of dataSet's fold \n","save_fold='datanp_vgg/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hDsLG8ryhclQ","colab":{}},"source":["def read_dir(dir_path) :\n","  \"\"\"return the differents categories, the images in these categories, and the number of images per category\"\"\"\n","  listDir = sorted(os.listdir(dir_path))\n","  sizes = []\n","  listFiles = []\n","  for d in listDir :\n","    files = sorted(os.listdir(dir_path+'/'+d))\n","    sizes += [len(files)]\n","    listFiles += [files]\n","  return listDir, listFiles, sizes\n","\n","\n","def create_zero_array(layer, n, nb_images):\n","  \"\"\"return a numpy array with zeros and with the needed scales\"\"\"\n","  \n","\n","  if layer == 'block5_pool' :\n","    X = np.zeros((nb_images, 7 + 10*n, 7 + 10*n, 512),dtype=np.float16)\n","    \n","  else :\n","    if n == 0 :\n","      X = np.zeros((nb_images, 4096),dtype=np.float16)\n","    else :\n","#       X = np.zeros((nb_images, -3 + 5*n, -3 + 5*n, 4096),dtype=np.float16) #???\n","        X = np.zeros((nb_images, 1 + 10*n, 1 + 10*n, 4096),dtype=np.float16)\n","  return X\n","  \n","\n","def complete_array(network2,dir_path, dir_save, listDir, listFiles, X_new, n, net3) :\n","  \"\"\"for all the images in listFiles, extract the features thanks to the network\"\"\"\n","  \"\"\"return a 4-dimensional array with a 3-dimensional array per image\"\"\"\n","  nb = 0\n","  cpt = 0\n","  t0 = time.time()\n","  for i in range (len(listDir)) :\n","    d = listDir[i]\n","    files = listFiles[i]\n","    print(d, len(files))\n","    nb += len(files)\n","    for f in files :\n","#       print(str(f))\n","      t1 = time.time()\n","      img = image.load_img(dir_path+'/'+d+'/'+f, target_size=(224 + 320*n, 224 + 320*n))\n","      x = image.img_to_array(img)\n","      x = np.expand_dims(x, axis=0)\n","#       print(x.shape)\n","      x = preprocess_input(x)\n","      \n","      temp = net3.predict(x)\n","#       print( temp.shape)\n","      \n","      features = network2.predict(x)\n","#       print(features.shape)\n","\n","      X_new[cpt,:] = features[0,:]\n","#       print(features.shape)\n","#       input()\n","      cpt += 1\n","      t2 = time.time()\n","      #print(str(cpt) + '/' + str(nb), t2 - t1)\n","  #np.save(dir_save, X_new)\n","  print('total', t2 - t0)\n","  return X_new\n","\n","def pool(X, p):\n","  \"\"\"return the numpy array X on which the pooling p was performed\"\"\"\n","  if(len(X.shape) == 2):\n","    return X\n","  if p == 'max' :  \n","    X_pool = np.max(X, axis = (1, 2)) \n","  elif p == 'mean' :\n","    X_pool = np.mean(X, axis = (1, 2)) \n","  else :\n","    X_pool = X\n","  return X_pool\n","\n","\n","def to_fully_conv(model):\n","  \"\"\"transforms the Convolutional Neural Network model into a Fully Convolutional Network\"\"\"\n","\n","  new_model = Sequential()\n","\n","  input_layer = InputLayer(input_shape=(None, None, 3), name=\"input_new\")\n","\n","  new_model.add(input_layer)\n","\n","  for layer in model.layers:\n","\n","      if \"Flatten\" in str(layer):\n","          flattened_ipt = True\n","          f_dim = layer.input_shape\n","          print(\"get into flatten\")\n","          continue\n","\n","      elif \"Dense\" in str(layer):\n","\n","          input_shape = layer.input_shape\n","          output_dim =  layer.get_weights()[1].shape[0]\n","          W,b = layer.get_weights()\n","\n","          if flattened_ipt:\n","              shape = (f_dim[1],f_dim[2],f_dim[3],output_dim)\n","              new_W = W.reshape(shape)\n","\n","              aa = np.array([new_W,b])\n","#               print(aa.shape)\n","              new_layer = Convolution2D(output_dim,\n","                                        (f_dim[1],f_dim[2]),\n","                                        #strides=(1,1),\n","                                        activation=layer.activation,\n","                                        padding='valid',\n","                                        weights=[new_W,b])\n","              flattened_ipt = False\n","\n","          else:\n","              shape = (1,1,input_shape[1],output_dim)\n","              new_W = W.reshape(shape)\n","\n","              new_layer = Convolution2D(output_dim,\n","                                        (1,1),\n","                                        #strides=(1,1),\n","                                        activation=layer.activation,\n","                                        padding='valid',\n","                                        weights=[new_W,b])\n","\n","\n","      else:\n","          new_layer = layer\n","      new_model.add(new_layer)\n","\n","  return new_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gisNgSS_oMcG","outputId":"30e838f7-f3bf-4d11-c792-c06a95e56bd4","executionInfo":{"status":"ok","timestamp":1557075260112,"user_tz":-120,"elapsed":928798,"user":{"displayName":"CHEN DANG","photoUrl":"https://lh4.googleusercontent.com/-ULtn-wp1jmI/AAAAAAAAAAI/AAAAAAAAAGY/2jasw42jxC8/s64/photo.jpg","userId":"10929130588901207444"}},"colab":{"base_uri":"https://localhost:8080/","height":1493}},"source":["#name of datasets\n","# docs=['chest_xray','miniMIT_Etus','kvasir-dataset-v2']\n","docs=['miniMIT_Etus']\n","#images scales\n","scales=[1,2, 0]\n","#layer's names\n","layers=['fc1','fc2','block5_pool']\n","# pooling = 'mean'\n","\n","\n","for layer in layers:\n","  #definition of the network used (VGG19)\n","  network = VGG19(weights='imagenet')#, input_shape=(fscale, fscale, 3))\n","  net2 = Model(inputs=network.input, outputs=network.get_layer('block5_pool').output)\n","\n","  network3 = Model(inputs=network.input, outputs=network.get_layer(layer).output)\n","  for doc in docs:\n","    for n in scales:\n","    \n","      if n >= 1 : \n","        network2 = to_fully_conv(network3)\n","        net3 = to_fully_conv(net2)\n","        \n","      else:\n","        network2=network3\n","        net3 = net2 #??????\n","      \n","      \n","        \n","      print( \"doc = \" + doc + \"layer = \" + layer + \" n = \" + str(n) )\n","      if doc == 'kvasir-dataset-v2':\n","        #path where all images are stocked\n","        dir_path_X =  dir_path_DataSet + doc \n","        #path where the output array will be saanpved\n","        path_save_X = dir_path_DataSet + doc + '/' + save_fold + doc + 'X_'+layer  +'(' + str(n) + ').npy'\n","        dir_save_Y  = dir_path_DataSet + doc + '/' + save_fold + doc + 'Y_'+layer  +'(' + str(n) + ').npy'\n","\n","        #creating numpy array from images\n","        listDir_X, listFiles_X, sizes_X = read_dir(dir_path_X)\n","        X = create_zero_array(layer, n, sum(sizes_X))\n","        X = complete_array(network2,dir_path_X, path_save_X, listDir_X, listFiles_X, X, n)\n","        # create Y and save it\n","        Y = np.zeros(shape=(np.sum(sizes_X)))  \n","\n","        init_size=0\n","        for i in range (len(listDir_X)):\n","          size=sizes_X[i]\n","          Y[init_size:init_size+size]=i\n","          init_size+=size \n","          \n","        x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","\n","      else:\n","        #path where train images and test images are stocked\n","        dir_path_train =  dir_path_DataSet + doc + '/train'\n","        dir_path_test  =  dir_path_DataSet + doc + '/test'\n","\n","        #path where the output array will be saved\n","        path_save_train = dir_path_DataSet + doc + '/' + save_fold + doc + 'X_train_'+ layer  +'(' + str(n) + ').npy'\n","        path_save_test  = dir_path_DataSet + doc + '/' + save_fold + doc + 'X_test_' + layer   +'(' + str(n) + ').npy'\n","\n","        #creating numpy array from images\n","        listDir_train, listFiles_train, sizes_train = read_dir(dir_path_train)\n","        listDir_test, listFiles_test, sizes_test = read_dir(dir_path_test)\n","\n","\n","        X_train = create_zero_array(layer, n, sum(sizes_train))\n","        X_test = create_zero_array(layer, n, sum(sizes_test))\n","        \n","        X_train = complete_array(network2,dir_path_train, path_save_train, listDir_train, listFiles_train, X_train, n,net3)        \n","        X_test = complete_array(network2,dir_path_test, path_save_test, listDir_test, listFiles_test, X_test, n, net3)\n","        \n","# #         pooling\n","#         X_train = pool(X_train, pooling)\n","#         X_test = pool(X_test, pooling)\n","        \n","        dir_save_Y_train = dir_path_DataSet + doc + '/' + save_fold + doc + 'Y_train_'+ layer +'(' + str(n) + ').npy'\n","        dir_save_Y_test  = dir_path_DataSet + doc + '/' + save_fold + doc + 'Y_test_' + layer +'(' + str(n) + ').npy'\n","    \n","        # create Y_train and save it\n","        Y_train = np.zeros(shape=(np.sum(sizes_train)))  \n","\n","        init_size=0\n","        for i in range (len(listDir_train)):\n","          size=sizes_train[i]\n","          Y_train[init_size:init_size+size]=i\n","          init_size+=size\n","\n","        # create Y_test and save it\n","        Y_test = np.zeros(shape=(np.sum(sizes_test)))  \n","\n","        init_size=0\n","        for i in range (len(listDir_test)):\n","          size=sizes_test[i]\n","          Y_test[init_size:init_size+size]=i\n","          init_size+=size\n","\n","\n","      #save data \n","      np.save(dir_save_Y_train, Y_train)\n","      np.save(dir_save_Y_test, Y_test)\n","      #save data \n","      np.save(path_save_train, X_train)\n","      np.save(path_save_test, X_test)\n","  \n","      \n","  \n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["get into flatten\n","doc = miniMIT_Etuslayer = fc1 n = 1\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 47.59290313720703\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 47.09196877479553\n","get into flatten\n","doc = miniMIT_Etuslayer = fc1 n = 2\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 102.76506662368774\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 102.1616678237915\n","doc = miniMIT_Etuslayer = fc1 n = 0\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 9.5138578414917\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 8.39560341835022\n","get into flatten\n","doc = miniMIT_Etuslayer = fc2 n = 1\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 48.705116987228394\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 48.10479474067688\n","get into flatten\n","doc = miniMIT_Etuslayer = fc2 n = 2\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 104.30459022521973\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 103.65865993499756\n","doc = miniMIT_Etuslayer = fc2 n = 0\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 9.368441343307495\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 8.89061427116394\n","doc = miniMIT_Etuslayer = block5_pool n = 1\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 35.67585015296936\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 35.034170627593994\n","doc = miniMIT_Etuslayer = block5_pool n = 2\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 81.83306407928467\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 81.16644310951233\n","doc = miniMIT_Etuslayer = block5_pool n = 0\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 8.784973382949829\n","bookstore 40\n","inside_bus 40\n","library 40\n","total 7.937650680541992\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Ji5K_LzvqtA","colab_type":"code","outputId":"7d450e18-6af0-45b6-ceef-1fd8f8646947","executionInfo":{"status":"ok","timestamp":1557075406628,"user_tz":-120,"elapsed":1462,"user":{"displayName":"CHEN DANG","photoUrl":"https://lh4.googleusercontent.com/-ULtn-wp1jmI/AAAAAAAAAAI/AAAAAAAAAGY/2jasw42jxC8/s64/photo.jpg","userId":"10929130588901207444"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_train.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(120, 7, 7, 512)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"WhuUHSqeksXO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}