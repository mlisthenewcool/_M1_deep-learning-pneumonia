{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "W4cZG18_Knqu",
    "outputId": "22cc84a3-42cc-4ad3-d82b-41382c0e0504"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "suYF7J0Eam32",
    "outputId": "9c22b8b1-98a6-4147-84c0-0ee243367605"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUnJIUpYYGz2"
   },
   "outputs": [],
   "source": [
    "def save_file(path,obj):\n",
    "    pickle_out = open(path,\"wb\")\n",
    "    pickle.dump(obj, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Igxbve2Naz5l"
   },
   "outputs": [],
   "source": [
    "#we don't need all descriptor to train Kmean or PCA,we can  just use some discriptors like 30 or 50 for each image\n",
    "k_descriptors=30\n",
    "\n",
    "#dataset's path \n",
    "dir_path_DataSet=''\n",
    "\n",
    "#fold name for saving files(kmean/PCA)\n",
    "fold_name=\"files/\"\n",
    "save_flod=''\n",
    "\n",
    "docs=['miniMIT_Etus','chest_xray','kvasir-dataset-v2']#'kvasir-dataset-v2',]#,'chest_xray','kvasir-dataset-v2']#,'miniMIT_Etus','kvasir-dataset-v2']\n",
    "scales=[0,1,2]\n",
    "layers=['block5_pool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "GGAKXPmGa3Q6",
    "outputId": "7aec6573-997a-432e-b248-e3efbc9472c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kvasir-dataset-v2 0 block5_pool\n",
      "float16\n",
      "(6400, 7, 7, 512)\n",
      "(6400, 30, 512)\n",
      "(6400, 30, 512)\n",
      "kvasir-dataset-v2 1 block5_pool\n",
      "float16\n",
      "(6400, 17, 17, 512)\n",
      "(6400, 30, 512)\n",
      "(6400, 60, 512)\n",
      "kvasir-dataset-v2 2 block5_pool\n",
      "float16\n",
      "(6400, 27, 27, 512)\n",
      "(6400, 30, 512)\n",
      "(6400, 90, 512)\n",
      "(576000, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhat/.local/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/farhat/.local/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA fit ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhat/.local/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca shape\n",
      "(576000, 128)\n",
      "Kmean1 64 ...\n",
      "Kmean2 256 ...\n",
      "end of  kvasir-dataset-v2 2 block5_pool\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    for layer in layers:\n",
    "        for n in scales:\n",
    "            print(doc, n , layer)\n",
    "            #laod_data, we dont need to load all data \n",
    "            X=np.load(dir_path_DataSet + save_flod + doc +'X_train_'+layer + '(' + str(n) + ').npy')\n",
    "            \n",
    "            #this just for miniMIT bc we don't have many descriptorts \n",
    "            if doc=='miniMIT_Etus':\n",
    "                k_descriptors=X.shape[1]*X.shape[2]\n",
    "                \n",
    "\n",
    "\n",
    "            #reshape (d1,d2,d3,d4) -> (d1,d2*d3,d4)\n",
    "            d=X.shape[1]*X.shape[2]\n",
    "            X = np.reshape(X,(X.shape[0],d,X.shape[3]))\n",
    "            \n",
    "            #we take only some discriptors\n",
    "            if doc!='miniMIT_Etus':\n",
    "                #take random descriptors\n",
    "                indexes = np.arange(d)\n",
    "                np.random.shuffle(indexes)\n",
    "                indexes=indexes[:k_descriptors]\n",
    "                X=X[:,indexes,:]\n",
    "                \n",
    "            \n",
    "        \n",
    "            if(n==0):\n",
    "                x_all=X\n",
    "            else:\n",
    "                x_all=np.concatenate((x_all,X),axis=1)\n",
    "                del X\n",
    "            #x_test_all=np.concatenate((x_test_all,x_test),axis=1)\n",
    "\n",
    "            print(x_all.shape)\n",
    "      \n",
    "      \n",
    "      \n",
    "        \n",
    "        x_all=np.reshape(x_all,(x_all.shape[0]*x_all.shape[1],x_all.shape[2]))\n",
    "        print(x_all.shape)\n",
    "        #normalization l2\n",
    "        x_all = sklearn.preprocessing.normalize(x_all, norm = 'l2', axis = 1)\n",
    "\n",
    "        #PCA \n",
    "        pca = PCA(n_components=128)\n",
    "        x_all = sklearn.preprocessing.normalize(x_all, norm = 'l2', axis = 1)\n",
    "        pca.fit(x_all)\n",
    "\n",
    "\n",
    "        #SAVE the PCA  pickle\n",
    "        save_file(ter_path+fold_name+doc+'_'+layer+'_'+'PCA.pickle',pca)\n",
    "\n",
    "        #transformation with PCA\n",
    "        print(\"PCA fit ...\")\n",
    "        x_train=pca.transform(x_all)\n",
    "        del x_all\n",
    "\n",
    "        #normalisation\n",
    "        x_train = sklearn.preprocessing.normalize(x_train, norm = 'l2', axis = 1)\n",
    "        print(\"pca shape\")\n",
    "        print(x_train.shape)\n",
    "\n",
    "        #kmeans 64 clusters\n",
    "        print(\"Kmean1 64 ...\")\n",
    "        kmeans1 = KMeans(n_clusters=64, random_state=0).fit(x_train)\n",
    "\n",
    "        #SAVE the kmeans1  pickle\n",
    "        save_file(ter_path+fold_name+doc+'_'+layer+'_'+'kmean1.pickle',kmeans1)\n",
    "\n",
    "        print(\"Kmean2 256 ...\")\n",
    "        #kmeans 256 clusters\n",
    "        kmeans2 = KMeans(n_clusters=256, random_state=0).fit(x_train)\n",
    "\n",
    "        #SAVE the kmeans2  pickle\n",
    "        save_file(ter_path+fold_name+doc+'_'+layer+'_'+'kmean2.pickle',kmeans2)\n",
    "        print('end of ',doc , n , layer)\n",
    "        \n",
    "print(\"END\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "k_mean.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
