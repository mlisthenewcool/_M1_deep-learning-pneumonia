{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qhG8deumhkP4",
    "outputId": "96c3ef4b-ee32-4460-d4cb-ba7d4f267a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AdMtzQw8hd2n",
    "outputId": "7c8ac16b-44e4-4672-ce52-54bce89ba668"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, ActivityRegularization, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv1D, MaxPooling1D, TimeDistributed\n",
    "from keras.layers import AveragePooling2D, Input\n",
    "from keras.utils import np_utils, normalize\n",
    "from keras.engine import InputLayer\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qA-lUBL_pCbg"
   },
   "outputs": [],
   "source": [
    "network = VGG19(weights='imagenet')#, input_shape=(fscale, fscale, 3))\n",
    "#print(network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnZbOJt1gNo9"
   },
   "outputs": [],
   "source": [
    "#dataset's path \n",
    "dir_path_DataSet='drive/My Drive/M2_IAAA/ter/DataSet/'\n",
    "\n",
    "#path of dataSet's fold \n",
    "save_fold='datanp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDsLG8ryhclQ"
   },
   "outputs": [],
   "source": [
    "def read_dir(dir_path) :\n",
    "  \"\"\"return the differents categories, the images in these categories, and the number of images per category\"\"\"\n",
    "  listDir = sorted(os.listdir(dir_path))\n",
    "  sizes = []\n",
    "  listFiles = []\n",
    "  for d in listDir :\n",
    "    files = sorted(os.listdir(dir_path+'/'+d))\n",
    "    sizes += [len(files)]\n",
    "    listFiles += [files]\n",
    "  return listDir, listFiles, sizes\n",
    "\n",
    "\n",
    "def create_zero_array(layer, n, nb_images):\n",
    "  \"\"\"return a numpy array with zeros and with the needed scales\"\"\"\n",
    "  \n",
    "  \n",
    "  if layer == 'block5_pool' :\n",
    "    X = np.zeros((nb_images, 7 + 10*n, 7 + 10*n, 512),dtype=np.float16)\n",
    "    \n",
    "  else :\n",
    "    if n == 0 :\n",
    "      X = np.zeros((nb_images, 4096),dtype=np.float16)\n",
    "    else :\n",
    "      X = np.zeros((nb_images, -3 + 5*n, -3 + 5*n, 4096),dtype=np.float16)\n",
    "  return X\n",
    "  \n",
    "\n",
    "def complete_array(network2,dir_path, dir_save, listDir, listFiles, X_new, n, net3) :\n",
    "  \"\"\"for all the images in listFiles, extract the features thanks to the network\"\"\"\n",
    "  \"\"\"return a 4-dimensional array with a 3-dimensional array per image\"\"\"\n",
    "  nb = 0\n",
    "  cpt = 0\n",
    "  t0 = time.time()\n",
    "  for i in range (len(listDir)) :\n",
    "    d = listDir[i]\n",
    "    files = listFiles[i]\n",
    "    print(d, len(files))\n",
    "    nb += len(files)\n",
    "    for f in files :\n",
    "      #print(str(f))\n",
    "      t1 = time.time()\n",
    "      img = image.load_img(dir_path+'/'+d+'/'+f, target_size=(224 + 320*n, 224 + 320*n))\n",
    "      x = image.img_to_array(img)\n",
    "      x = np.expand_dims(x, axis=0)\n",
    "      print(x.shape)\n",
    "      x = preprocess_input(x)\n",
    "      \n",
    "      temp = net3.predict(x)\n",
    "      print( temp.shape)\n",
    "      \n",
    "      features = network2.predict(x)\n",
    "      print(features.shape)\n",
    "\n",
    "      X_new[cpt,:] = features[0,:]\n",
    "      print(features.shape)\n",
    "      input()\n",
    "      cpt += 1\n",
    "      t2 = time.time()\n",
    "      #print(str(cpt) + '/' + str(nb), t2 - t1)\n",
    "  #np.save(dir_save, X_new)\n",
    "  print('total', t2 - t0)\n",
    "  return X_new\n",
    "\n",
    "def to_fully_conv(model):\n",
    "  \"\"\"transforms the Convolutional Neural Network model into a Fully Convolutional Network\"\"\"\n",
    "\n",
    "  new_model = Sequential()\n",
    "\n",
    "  input_layer = InputLayer(input_shape=(None, None, 3), name=\"input_new\")\n",
    "\n",
    "  new_model.add(input_layer)\n",
    "\n",
    "  for layer in model.layers:\n",
    "\n",
    "      if \"Flatten\" in str(layer):\n",
    "          flattened_ipt = True\n",
    "          f_dim = layer.input_shape\n",
    "\n",
    "      elif \"Dense\" in str(layer):\n",
    "\n",
    "          input_shape = layer.input_shape\n",
    "          output_dim =  layer.get_weights()[1].shape[0]\n",
    "          W,b = layer.get_weights()\n",
    "\n",
    "          if flattened_ipt:\n",
    "              shape = (f_dim[1],f_dim[2],f_dim[3],output_dim)\n",
    "              new_W = W.reshape(shape)\n",
    "              print('ouput_dim',output_dim)\n",
    "              print('f_dim',f_dim)\n",
    "              print('ouput_dim',output_dim)\n",
    "              print(new_W.shape)\n",
    "\n",
    "              aa = np.array([new_W,b])\n",
    "              print(aa.shape)\n",
    "              new_layer = Convolution2D(output_dim,\n",
    "                                        (f_dim[1],f_dim[2]),\n",
    "                                        #strides=(1,1),\n",
    "                                        activation=layer.activation,\n",
    "                                        padding='same',\n",
    "                                        weights=[new_W,b])\n",
    "              flattened_ipt = False\n",
    "\n",
    "          else:\n",
    "              shape = (1,1,input_shape[1],output_dim)\n",
    "              new_W = W.reshape(shape)\n",
    "              \n",
    "              print('2ouput_dim',output_dim)\n",
    "              print('2f_dim',f_dim)\n",
    "              print('2ouput_dim',output_dim)\n",
    "\n",
    "              new_layer = Convolution2D(output_dim,\n",
    "                                        (1,1),\n",
    "                                        #strides=(1,1),\n",
    "                                        activation=layer.activation,\n",
    "                                        padding='same',\n",
    "                                        weights=[new_W,b])\n",
    "\n",
    "\n",
    "      else:\n",
    "          new_layer = layer\n",
    "\n",
    "      new_model.add(new_layer)\n",
    "\n",
    "  return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gisNgSS_oMcG"
   },
   "outputs": [],
   "source": [
    "#name of datasets\n",
    "docs=['chest_xray','miniMIT_Etus','kvasir-dataset-v2']\n",
    "#images scales\n",
    "scales=[0,1,2]\n",
    "#layer's names\n",
    "layers=['fc1','fc2','block5_pool']\n",
    "\n",
    "\n",
    "\n",
    "for layer in layers:\n",
    "  #definition of the network used (VGG19)\n",
    "  network = VGG19(weights='imagenet')#, input_shape=(fscale, fscale, 3))\n",
    "  net2 = Model(inputs=network.input, outputs=network.get_layer('block5_pool').output)\n",
    "\n",
    "  network3 = Model(inputs=network.input, outputs=network.get_layer(layer).output)\n",
    "  for doc in docs:\n",
    "    for n in scales:\n",
    "    \n",
    "      if n >= 1 : \n",
    "        network2 = to_fully_conv(network3)\n",
    "        net3 = to_fully_conv(net2)\n",
    "        print(network2.summary())\n",
    "      else:\n",
    "        network2=network3\n",
    "      \n",
    "      \n",
    "        \n",
    "      print( \"doc = \" + doc + \"layer = \" + layer + \" n = \" + str(n) )\n",
    "      if doc == 'kvasir-dataset-v2':\n",
    "        #path where all images are stocked\n",
    "        dir_path_X =  dir_path_DataSet + doc \n",
    "        #path where the output array will be saanpved\n",
    "        path_save_X = dir_path_DataSet + save_fold + doc + 'X_'+layer  +'(' + str(n) + ').npy'\n",
    "        dir_save_Y  = dir_path_DataSet + save_fold + doc + 'Y_'+layer  +'(' + str(n) + ').npy'\n",
    "\n",
    "        #creating numpy array from images\n",
    "        listDir_X, listFiles_X, sizes_X = read_dir(dir_path_X)\n",
    "        X = create_zero_array(layer, n, sum(sizes_X))\n",
    "        X = complete_array(network2,dir_path_X, path_save_X, listDir_X, listFiles_X, X, n)\n",
    "        # create Y and save it\n",
    "        Y = np.zeros(shape=(np.sum(sizes_X)))  \n",
    "\n",
    "        init_size=0\n",
    "        for i in range (len(listDir_X)):\n",
    "          size=sizes_X[i]\n",
    "          Y[init_size:init_size+size]=i\n",
    "          init_size+=size \n",
    "          \n",
    "        x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "      else:\n",
    "        #path where train images and test images are stocked\n",
    "        dir_path_train =  dir_path_DataSet + doc + '/train'\n",
    "        dir_path_test  =  dir_path_DataSet + doc + '/test'\n",
    "\n",
    "        #path where the output array will be saved\n",
    "        path_save_train = dir_path_DataSet + save_fold + doc + 'X_train_'+ layer  +'(' + str(n) + ').npy'\n",
    "        path_save_test  = dir_path_DataSet + save_fold + doc + 'X_test_' + layer   +'(' + str(n) + ').npy'\n",
    "\n",
    "        #creating numpy array from images\n",
    "        listDir_train, listFiles_train, sizes_train = read_dir(dir_path_train)\n",
    "        listDir_test, listFiles_test, sizes_test = read_dir(dir_path_test)\n",
    "\n",
    "\n",
    "        X_train = create_zero_array(layer, n, sum(sizes_train))\n",
    "        X_test = create_zero_array(layer, n, sum(sizes_test))\n",
    "        \n",
    "\n",
    "        X_train = complete_array(network2,dir_path_train, path_save_train, listDir_train, listFiles_train, X_train, n,net3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_test = complete_array(network2,dir_path_test, path_save_test, listDir_test, listFiles_test, X_test, n)\n",
    "        \n",
    "        dir_save_Y_train = dir_path_DataSet + save_fold + doc + 'Y_train_'+ layer +'(' + str(n) + ').npy'\n",
    "        dir_save_Y_test  = dir_path_DataSet + save_fold + doc + 'Y_test_' + layer +'(' + str(n) + ').npy'\n",
    "\n",
    "        # create Y_train and save it\n",
    "        Y_train = np.zeros(shape=(np.sum(sizes_train)))  \n",
    "\n",
    "        init_size=0\n",
    "        for i in range (len(listDir_train)):\n",
    "          size=sizes_train[i]\n",
    "          Y_train[init_size:init_size+size]=i\n",
    "          init_size+=size\n",
    "\n",
    "        # create Y_test and save it\n",
    "        Y_test = np.zeros(shape=(np.sum(sizes_test)))  \n",
    "\n",
    "        init_size=0\n",
    "        for i in range (len(listDir_test)):\n",
    "          size=sizes_test[i]\n",
    "          Y_test[init_size:init_size+size]=i\n",
    "          init_size+=size\n",
    "\n",
    "\n",
    "      #save data \n",
    "      np.save(dir_save_Y_train, Y_train)\n",
    "      np.save(dir_save_Y_test, Y_test)\n",
    "  \n",
    "      \n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CreateDataVGG19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
