# -*- coding: utf-8 -*-
"""pneumonia_predict_with_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QLp_KlwCuNqUpxfVJ69KVcVTx0_gpM1W
"""

#from google.colab import drive
#drive.mount('drive')

"""# Libraries"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon May 13 19:35:11 2019

@author: hippolyte
"""

import os
import h5py
import imgaug.augmenters as iaa
import numpy as np
import tensorflow as tf
import imgaug as aug
import datetime
import pickle

from keras.applications import vgg16, vgg19
from keras.models import Model, Sequential, load_model, save_model
from keras.layers import Input, Conv2D, SeparableConv2D, MaxPooling2D, Dense, Dropout, Flatten
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.engine import InputLayer

from matplotlib import pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix

"""# Variables"""

# root path of the project
#ROOT_PATH = 'drive/My Drive/master1/medical_image_recognition/'
ROOT_PATH = '/home/hippolyte/Documents/universite/m1/TER/'

# name of the dataset
DATASET_NAME = 'chest_xray'

# define image extensions we accept
IMAGE_EXTENSIONS = ['*.jpg', '*.jpeg']

# dimensions for the images
HEIGHT, WIDTH, CHANNELS = 224, 224, 3

# output var to see infos
VERBOSE = True

# paths to each directory
directory_list = None #['test', 'train', 'val']

# labels for each directory
# if setup to None, detect according to subdirectories inside each directory
label_list = None #['NORMAL', 'PNEUMONIA']

# image augmentation sequence
# WE DON'T USE THAT YET
seq = iaa.OneOf([
    iaa.Fliplr(), # horizontal flips
    iaa.Affine(rotate=30)]) # rotation

# the next instructions are used to make results reproducible
seed = 1234
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(seed)
tf.set_random_seed(seed)
aug.seed(seed)

################################################################################
# DON'T TOUCH THE VARIABLES UNDER
################################################################################

# these are helpers to ease consistency
DATASET_PATH = ROOT_PATH + 'datasets/' + DATASET_NAME + '/'
MODEL_PATH = ROOT_PATH + 'models/' + DATASET_NAME + '/'
ARRAY_PATH = ROOT_PATH + 'arrays/' + DATASET_NAME + '/'

# create the directories to save arrays and models if they don't exist
os.system('mkdir -p {} {}'.format(MODEL_PATH, ARRAY_PATH))

# list directories according to the list defined on top
if directory_list is None:
    DIRECTORIES = sorted([d for d in os.listdir(DATASET_PATH)])
else:
    DIRECTORIES = directory_list

# get the labels inside the first directory
# of course, it should be the same in every directory
if label_list is None:
    LABELS = sorted(os.listdir(DATASET_PATH + DIRECTORIES[0]))
else:
    LABELS = label_list

NUM_LABELS = len(LABELS)

# get the paths
PATHS = dict()
for cur_dir in DIRECTORIES:
    PATHS[cur_dir] = DATASET_PATH + cur_dir + '/'

"""# Data function"""

def get_data(directory: str):
    """
    TODO
    """
    with h5py.File(ARRAY_PATH + directory + '.h5', 'r') as file:
        start = datetime.datetime.now()

        # assuming the file contains same amount of image and labels
        num_images = np.ceil(len(file.keys()) / 2).astype(int)
        print('Reading {} images from {} ...'.format(num_images, directory))

        # define arrays holding data and labels
        data = np.zeros((num_images, HEIGHT, WIDTH, CHANNELS), dtype=np.float32)
        labels = np.zeros((num_images, NUM_LABELS), dtype=np.float32)

        # iterate over all images
        # format is x0 y0 for the first image and so on
        for image_index in range(num_images):
            image = file['x' + str(image_index)]
            label = file['y' + str(image_index)]

            data[image_index] = image
            labels[image_index] = label

    # shapes
    print('Shapes. Data: {} Labels: {}'.format(data.shape,labels.shape))
    #data = np.array(data).astype(np.float32)
    #labels = np.array(labels)
    #print('Final shapes. {}   {}'.format(data.shape, labels.shape))

    end = datetime.datetime.now()
    print('Done in {} seconds.'.format((end-start).seconds))

    return data, labels

"""# Convolutional Neural Network functions"""

def get_cnn_model(model_name: str, **kwargs):
    """
    TODO
    """
    if model_name == 'vgg16':
        return create_vgg16(kwargs)

    #elif model_name == 'vgg16custom':
    #    return create_vgg16custom(kwargs)

    elif model_name == 'vgg19':
        return create_vgg19(kwargs)

    elif model_name == 'vgg19custom':
        return create_vgg19custom(kwargs)

    elif model_name == 'xception':
        return create_xception(kwargs)

    else:
        raise ValueError(
                'The model {} isn\'t implemented yet'.format(model_name))

################################################################################
def create_vgg16():
    """
    TODO
    """
    base_model = vgg16.VGG16(include_top=False,
                             weights='imagenet',
                             input_shape=(HEIGHT, WIDTH, CHANNELS))

    model = Sequential()
    model.add(base_model)

    # add classification block
    model.add(Flatten(name='flatten'))
    model.add(Dense(1024, activation='relu', name='fc1'))
    model.add(Dropout(0.7, name='dropout1'))
    model.add(Dense(512, activation='relu', name='fc2'))
    model.add(Dropout(0.5, name='dropout2'))
    model.add(Dense(NUM_LABELS, activation='softmax', name='predictions'))

    # see the base model architecture
    #base_model.summary()

    return model

################################################################################
def create_vgg19(_model_name,
                 _frozen_layers=None,
                 _include_top=True):
    """
    TODO
    """

    # create the VGG model
    vgg19_model = vgg19.VGG19(include_top=_include_top,
                              weights='imagenet',
                              input_shape=(HEIGHT, WIDTH, CHANNELS))

    #vgg19_model.summary()

    # create a sequential model
    frozen_layers_str = '_'.join(_frozen_layers)
    m_name = 'vgg19_' + frozen_layers_str
    model = Sequential(name=m_name)

    # add input layer
    model.add(InputLayer(input_shape=(HEIGHT, WIDTH, CHANNELS)))

    # convert vgg19 to sequential and remove the last layer
    for layer in vgg19_model.layers[:-1]:
        model.add(layer)

    #model.summary()

    # add last layer
    model.add(Dense(NUM_LABELS, activation='softmax', name='predictions'))

    #model.summary()

    # FREEZE PART
    if _frozen_layers is None:
        print('Everything is trainable')
        return model

    # freeze the first 5 blocks
    if 'blocks' in _frozen_layers:
        for i, layer in enumerate(model.layers):
            # freeze everything except fc1 and fc2
            if layer.name not in ['fc1', 'fc2']:
                #print('Not trainable', i, layer.name)
                layer.trainable = False

        print('Froze all blocks before classification one')

    # freeze fc1
    if 'fc1' in _frozen_layers:
        model.get_layer('fc1').trainable=False
        print('Froze fc1')

    # freeze fc2
    if 'fc2' in _frozen_layers:
        model.get_layer('fc2').trainable=False
        print('Froze fc2')

    return model

################################################################################
def create_vgg19custom(_model_name,
                       _frozen_layers=[]):
    """
    TODO
    """

    # create the VGG model
    vgg19_model = vgg19.VGG19(include_top=False,
                              weights='imagenet',
                              input_shape=(HEIGHT, WIDTH, CHANNELS))

    # freeze the layers according to `frozen_layers`
    for i, layer in enumerate(model.layers):
        if layer.name in frozen_layers:
            print('Not trainable', i, layer.name)
            layer.trainable = False
        else:
            print('Trainable', i, layer.name)
            layer.trainable = True


    # create a sequential model and add the vgg19 model at bottom
    model = Sequential(name='vgg19_custom_fc1_4096_fc2_4096')
    model.add(vgg19_model)

    # add classification block
    model.add(Flatten(name='flatten'))
    model.add(Dense(4096, activation='relu', name='fc1')) #1024
    model.add(Dropout(0.7, name='dropout1'))              #notpresent
    model.add(Dense(4096, activation='relu', name='fc2'))  #512
    model.add(Dropout(0.5, name='dropout2'))              #notpresent
    model.add(Dense(NUM_LABELS, activation='softmax', name='predictions'))

    #weights_list = model.get_weights()
    #for i, layer in enumerate(model.get_layer('vgg19').layers):
    #    print(i, layer.name)

    #base_model.summary()
    #model.summary()

    return model

def compile_model(model, model_name, learning_rate=1e-5):
    """
    TODO
    """
    loss_type = 'binary' if NUM_LABELS == 2 else 'categorical'
    loss_type += '_crossentropy'
    optimizer = Adam(lr=learning_rate, decay=1e-5)

    model.compile(loss=loss_type,
                  metrics=['accuracy'],
                  optimizer=optimizer)

    return model

def train_model(model,
                model_name,
                train_data,
                train_labels,
                val_data,
                val_labels,
                epoch=20,
                batch_size=16,
                metric='val_loss',
                save_best_only=True,
                save_weights_only=True,
                stop_after=5,
                save_history=True):
    """
    TODO
    """
    early_stopping = EarlyStopping(patience=stop_after,
                                   monitor=metric,
                                   restore_best_weights=True)

    check_path = MODEL_PATH + model_name + '_{epoch:02d}_{val_acc:2f}.hdf5'
    checkpoint = ModelCheckpoint(check_path,
                                 monitor=metric,
                                 verbose=1,
                                 save_best_only=save_best_only,
                                 save_weights_only=save_weights_only)

    # steps per epoch
    #train_steps = len(train_data) // batch_size
    #val_steps = len(val_data)

    print('Training {} model during {} epochs'.format(model_name, epoch))
    #print('{} train and {} val steps per epoch.'.format(train_steps, val_steps))

    # here we are, we'll train the model
    history = model.fit(x=train_data,
                        y=train_labels,
                        batch_size=batch_size,
                        epochs=epoch,
                        verbose=1,
                        callbacks=[early_stopping, checkpoint],
                        validation_split=0.0,
                        validation_data=(val_data, val_labels),
                        shuffle=True,
                        class_weight={0: 1.0, 1: 0.4}) # TODO

    # save history
    if save_history is True:
        history_path = MODEL_PATH + model_name + '_history'
        print('\n\nSaving history into {} ...'.format(history_path))
        with open(history_path, 'wb') as history_file:
            pickle.dump(history.history, history_file)

    return model, history

def test_model(model, model_name, test_data, test_labels, batch_size=16):
    """
    TODO
    """
    test_loss, test_score = model.evaluate(test_data,
                                           test_labels,
                                           batch_size=16)

    print('Results for {} model.'.format(model_name))
    print('Loss : {}'.format(test_loss))
    print('Score : {}'.format(test_score))

    # predictions
    preds = model.predict(test_data, batch_size=16)
    preds = np.argmax(preds, axis=-1)

    # original labels
    orig_test_labels = np.argmax(test_labels, axis=-1)

    # confusion matrix
    cm  = confusion_matrix(orig_test_labels, preds)

    # metrics
    recall = np.diag(cm) / np.sum(cm, axis=1)
    precision = np.diag(cm) / np.sum(cm, axis=0)
    accuracy = np.diag(cm) / np.sum(cm)

    tn, fp, fn, tp = cm.ravel()
    precision_ = tp / (tp + fp)
    recall_ =    tp / (tp + fn)
    accuracy_ = (tp + tn) / (tn + fp + fn + tp)

    print("Recall of the model is {} - {:.5f}".format(recall, recall_))
    print("Precision of the model is {} - {:.5f}".format(precision, precision_))
    print("Accuracy of the model is {} - {:.5f}".format(accuracy, accuracy_))

    # plot confusion matrix
    plt.figure()
    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)
    plt.xticks(range(NUM_LABELS), LABELS, fontsize=16)
    plt.yticks(range(NUM_LABELS), LABELS, fontsize=16)
    plt.show()

"""# Let's do it !

## Data part
"""

data, labels = dict(), dict()

for directory in DIRECTORIES:
    # get data and labels
    data[directory], labels[directory] = get_data(directory)

    # plot an image with label
    if False:
        plt.imshow(data[directory][0])
        plt.show()
        print(labels[directory][0])

"""## Test : reload model"""

#del model, history, loaded_model
MODEL_NAME = 'vgg19'
model = load_model(MODEL_PATH + '1_vgg19_/06_0.866987.hdf5')
model.summary()

"""## CNN results"""

# normally, `restore_best_weights` in early_stoping callback should have work
# set to True if not
if False:
    # choose the weights to load into model
    BEST_WEIGHTS = MODEL_PATH + 'vgg19_01_0.878205.hdf5'
    model.load_weights(BEST_WEIGHTS)

# let's test our model !
test_model(model, MODEL_NAME, data['test'], labels['test'], batch_size=1)

"""## Plot history"""

# load the history
history_path = MODEL_PATH + '1_vgg19_/' + 'history'
print('Loading history from {} ...'.format(history_path))
history = pickle.load(open(history_path, 'rb'))

fig, axes = plt.subplots(1, 2, figsize=(12, 4)) #, constrained_layout=True)

# accuracy
axes[0].plot(history['acc'])
axes[0].plot(history['val_acc'])
axes[0].set_title('Model Accuracy')
axes[0].set_ylabel('Accuracy')
axes[0].set_xlabel('Epochs')
axes[0].legend(['train', 'val'])

# loss
axes[1].plot(history['loss'])
axes[1].plot(history['val_loss'])
axes[1].set_title('Model Loss')
axes[1].set_ylabel('Loss')
axes[1].set_xlabel('Epochs')
axes[1].legend(['train', 'val'])
plt.show()
